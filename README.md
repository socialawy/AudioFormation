
# üè≠ AudioFormation

## Description

**Production audio pipeline: Voice, SFX, Music, Mix, Export.**

Companion to VideoFormation (same architecture, different domain).

### Philosophy (Mirrors VideoFormation)

Principle	| Implementation
-----------|---------------
Single Source of Truth	| project.json governs everything
Validation Gates	| Hard gates before generation, mixing, export
Automation First	| CLI drives pipeline; dashboard is optional
Engine Agnostic	| Swap TTS/music engines without touching project files
Hardware Aware	| Auto-detects GPU, suggests optimal engine
Bilingual First	| Arabic + English as primary languages

## Quick Start

```bash
# Install
pip install -e ".[dev,server]"

# Create a project
audioformation new "MY_NOVEL"

# Start the Dashboard
audioformation serve
# Open http://localhost:4001 in your browser
```

Or use the CLI:

```bash
# Ingest text
audioformation ingest MY_NOVEL --source ./chapters/

# Generate (edge-tts with gTTS fallback)
audioformation generate MY_NOVEL --engine edge

# Mix (Voice + Music + Ducking)
audioformation mix MY_NOVEL

# Export
audioformation export MY_NOVEL --format m4b
```

## Requirements
Python 3.11+
ffmpeg on PATH
Optional: NVIDIA GPU with 4GB+ VRAM for XTTS voice cloning

## Features
Feature | Status
-------|-------
Edge TTS (free, Arabic + English) | ‚úÖ BUILT
SSML direction mapping | ‚úÖ BUILT
Text chunking (breath-group) | ‚úÖ BUILT
Per-chunk QC scanning | ‚úÖ BUILT
LUFS normalization | ‚úÖ BUILT
MP3 export with manifest | ‚úÖ BUILT
Arabic diacritics detection | ‚úÖ BUILT
Engine fallback chain (edge-tts ‚Üí gTTS) | ‚úÖ BUILT
XTTS v2 engine adapter | ‚úÖ BUILT
ElevenLabs engine adapter | ‚úÖ BUILT
Multi-speaker dialogue | ‚úÖ BUILT
Ambient pad generation | ‚úÖ BUILT
VAD-based ducking | ‚úÖ BUILT
M4B audiobook export | ‚úÖ BUILT
Web dashboard | ‚úÖ BUILT

## Architecture

AudioFormation follows a modular pipeline architecture with five core domains:

```
audioformation CLI ‚Üí Pipeline State Machine
‚îú‚îÄ‚îÄ TTS Engines (edge, gtts, xtts, elevenlabs) ‚úÖ IMPLEMENTED
‚îú‚îÄ‚îÄ Audio Processor (normalize, trim, stitch) ‚úÖ IMPLEMENTED  
‚îú‚îÄ‚îÄ Ambient Composer (pad generation) ‚úÖ IMPLEMENTED
‚îú‚îÄ‚îÄ Mixer (multi-track, VAD ducking) ‚úÖ IMPLEMENTED
‚îú‚îÄ‚îÄ QC Scanner (per-chunk quality) ‚úÖ IMPLEMENTED
‚îî‚îÄ‚îÄ Exporter (MP3/M4B + manifest) ‚úÖ IMPLEMENTED
```

### Implementation Status

- ‚úÖ **Phase 1 Complete**: Core TTS pipeline, QC, audio processing
- ‚úÖ **Phase 2 Complete**: Cloud TTS adapters, voice cloning, multi-speaker, CLI tools
- ‚úÖ **Phase 3 Complete**: Mixer with ducking, M4B export, web interface (Editor/Mix)
- ‚è≥ **Phase 4 Future**: Algorithmic composition, advanced features

## Dashboard

The dashboard (`audioformation serve`) provides a visual interface for:
*   **Project Management**: Create and list projects.
*   **Editor**: Configure generation settings, edit chapter metadata, trigger generation per-chapter.
*   **Mix & Review**: Visualize audio waveforms (`wavesurfer.js`), play back generated/mixed audio, trigger the mixing pipeline.

## Testing
```bash
pip install -e ".[dev]"
pytest -v
# or
pytest --cov=src --cov-report=term-missing
```

## Contributing
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License
MIT

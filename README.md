# ðŸ­ AudioFormation

## Description

**Production audio pipeline: Voice, SFX, Music, Mix, Export.**

Companion to VideoFormation (same architecture, different domain).

### Philosophy (Mirrors VideoFormation)

Principle	| Implementation
-----------|---------------
Single Source of Truth	| project.json governs everything
Validation Gates	| Hard gates before generation, mixing, export
Automation First	| CLI drives pipeline; dashboard is optional
Engine Agnostic	| Swap TTS/music engines without touching project files
Hardware Aware	| Auto-detects GPU, suggests optimal engine
Bilingual First	| Arabic + English as primary languages

## Quick Start

```bash
# Install
pip install -e ".[dev]"

# Set up pre-commit hooks (recommended)
pre-commit install

# Verify
audioformation --version
audioformation hardware

# Create a project
audioformation new "MY_NOVEL"

# Add text files
cp chapters/*.txt PROJECTS/MY_NOVEL/01_TEXT/chapters/

# Or ingest from a directory
audioformation ingest MY_NOVEL --source ./chapters/

# Validate
audioformation validate MY_NOVEL

# Generate audio (edge-tts with gTTS fallback)
audioformation generate MY_NOVEL --engine edge
# Falls back to gTTS automatically if edge-tts fails (403/500 errors)

# Check quality
audioformation qc MY_NOVEL --report

# Process (normalize + trim)
audioformation process MY_NOVEL

# Export
audioformation export MY_NOVEL --format mp3

# Or run the full pipeline
audioformation run MY_NOVEL --all
```

## Quick Generation (No Project)

```bash
# English
audioformation quick "Hello world" --voice en-US-GuyNeural -o hello.mp3

# Arabic
audioformation quick "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…" --voice ar-SA-HamedNeural -o hello_ar.mp3

# From stdin
echo "Ù…Ø±Ø­Ø¨Ø§" | audioformation quick --voice ar-SA-HamedNeural
```
## Requirements
Python 3.11+
ffmpeg on PATH
Optional: NVIDIA GPU with 4GB+ VRAM for XTTS voice cloning

## Engine Support
- **edge-tts**: Free, excellent Arabic voices, upgraded to v7 for DRM compatibility
- **gTTS**: Emergency fallback, activates automatically on edge-tts failures
- **XTTS v2**: Local voice cloning, engine adapter built (requires GPU, `pip install -e ".[xtts]"`)
- **ElevenLabs**: Cloud TTS adapter built (requires API key, `pip install -e ".[cloud]"`)
- **Cloud engines**: OpenAI TTS, Gemini TTS (Phase 3, API keys required)

## Features
Feature | Status
-------|-------
Edge TTS (free, Arabic + English) | âœ… BUILT
SSML direction mapping | âœ… BUILT
Text chunking (breath-group) | âœ… BUILT
Per-chunk QC scanning | âœ… BUILT
LUFS normalization | âœ… BUILT
MP3 export with manifest | âœ… BUILT
Arabic diacritics detection | âœ… BUILT
Engine fallback chain (edge-tts â†’ gTTS) | âœ… BUILT
gTTS emergency fallback | âœ… BUILT
XTTS v2 engine adapter | âœ… BUILT
ElevenLabs engine adapter | âœ… BUILT
Multi-speaker dialogue | âœ… BUILT
Ambient pad generation | âœ… BUILT
CLI Tools (Cast, Compose, Preview) | âœ… BUILT
VAD-based ducking | â³ PHASE 3
M4B audiobook export | â³ PHASE 3
Web dashboard | â³ PHASE 3

## Architecture

AudioFormation follows a modular pipeline architecture with five core domains:

```
audioformation CLI â†’ Pipeline State Machine
â”œâ”€â”€ TTS Engines (edge, gtts, xtts, elevenlabs) âœ… IMPLEMENTED
â”œâ”€â”€ Audio Processor (normalize, trim, stitch) âœ… IMPLEMENTED  
â”œâ”€â”€ Ambient Composer (pad generation) âœ… IMPLEMENTED
â”œâ”€â”€ QC Scanner (per-chunk quality) âœ… IMPLEMENTED
â””â”€â”€ Exporter (MP3 + manifest) âœ… IMPLEMENTED
```

### Implementation Status

- âœ… **Phase 1 Complete**: Core TTS pipeline, QC, audio processing
- âœ… **Phase 2 Complete**: Cloud TTS adapters, voice cloning, multi-speaker, CLI tools
- â³ **Phase 3 Planned**: Mixer with ducking, M4B export, web interface
- â³ **Phase 4 Future**: Algorithmic composition, advanced features

### Current Capabilities

- âœ… Multi-engine TTS with automatic fallback
- âœ… Arabic text processing and chunking
- âœ… Audio quality control and normalization
- âœ… Multi-speaker chapter generation
- âœ… Project validation and error reporting
- âœ… CLI tools for character management, rapid previewing, and ambient music

## Pipeline

```mermaid
flowchart LR
    A[Bootstrap] --> B[Ingest]
    B --> C[Validate]
    C --> D[Generate]
    D --> E[QC]
    E --> F[Process]
    F --> G[Compose]
    G --> H[Mix]
    H --> I[QC Final]
    I --> J[Export]
    
    style C fill:#ff6b6b,stroke:#333
    style I fill:#ff6b6b,stroke:#333
```

- **[GATE]** = Hard validation gates (must pass)
- **[AUTO]** = Automatic retry on failure (engine fallback)
## Project Structure
- Each project lives under PROJECTS/ with a standard directory layout:
```text
PROJECTS/MY_NOVEL/
â”œâ”€â”€ project.json           # Single source of truth
â”œâ”€â”€ pipeline-status.json   # Execution state (chunk-level)
â”œâ”€â”€ 00_CONFIG/             # Hardware, engines, characters
â”œâ”€â”€ 01_TEXT/chapters/      # Source text files
â”œâ”€â”€ 02_VOICES/references/  # Voice cloning samples
â”œâ”€â”€ 03_GENERATED/          # TTS output (raw + processed)
â”œâ”€â”€ 04_SFX/                # Sound effects
â”œâ”€â”€ 05_MUSIC/              # Background music
â”œâ”€â”€ 06_MIX/                # Mix sessions + renders
â””â”€â”€ 07_EXPORT/             # Final MP3/M4B + manifest
```

## Testing
```bash
pip install -e ".[dev]"
pytest -v
```

## Current Status
- âœ… **Phase 1 Complete**: All core functionality implemented and tested
- âœ… **Engine Fallback**: edge-tts â†’ gTTS automatic fallback for robust generation
- âœ… **Arabic Support**: Full Arabic text processing with diacritics detection
- âœ… **Multi-Speaker**: Per-segment character resolution with engine-specific routing
- âœ… **XTTS Engine**: Adapter built with VRAM management
- âœ… **ElevenLabs Engine**: Cloud adapter built (API key required)
- âœ… **Phase 2 Complete**: `cast`, `compose`, `preview`, `compare` commands built and tested
- âœ… **Testing**: 100% pass rate (320/320 tests)
- â³ **Phase 3 Planned**: Mixer with ducking, M4B export, QC Final, web interface

## Known Issues & Limitations

| Issue | Impact | Workaround | Planned Fix |
|:---|:---|:---|:---|
| **Arabic SSML** | Direction config (pace/energy/emotion) not applied for Arabic edge-tts voices | Set `"edge_tts_ssml": false` in project.json | Alternative: Use XTTS for expressive Arabic narration |
| **Mishkal Quality** | Some Arabic words incorrectly diacritized, causing mispronunciation | Manual review of `.diacritized.txt` files | Evaluate CAMeL Tools or hybrid approach |
| **Robotic Tone** | Edge-tts Arabic voices lack narrative expressiveness | Use direction config with non-Arabic voices, or XTTS | Research: Fine-tuned XTTS for Arabic narration |
| **Session Length** | Tonal drift possible in very long generation sessions (>100 chunks) | Break into sessions: â‰¤50 chunks optimal, â‰¤100 chunks acceptable | Phase 3: Session management + quality gates |
| **click.echo coupling** | Library code (`generate.py`) uses `click.echo()` directly â€” will block Phase 3 server | Use CLI only (no server yet) | Decouple to callback/logging pattern before server work |

## Roadmap

### Phase 3 (Planned)
- Multi-track mixer with VAD-based ducking (`audio/mixer.py`)
- QC Final gate (depends on mixer output)
- M4B audiobook export with chapter markers
- FastAPI server + web dashboard with wavesurfer.js timeline
- FXForge (SFX domain â€” procedural + sample import)

### Phase 4 (Future)
- Algorithmic composition (ComposeEngine Tier 3)
- PyInstaller packaging for standalone distribution
- FishAudio-S1 / IndexTTS evaluation
- VideoFormation integration

## Contributing
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License
MIT
